{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Part1 : 탐색적 데이터 분석(Exploratory Data Analysis - EDA)","metadata":{}},{"cell_type":"markdown","source":"## 1. 라이브러리 및 데이터 불러오기","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-22T04:57:01.257018Z","iopub.execute_input":"2022-07-22T04:57:01.257800Z","iopub.status.idle":"2022-07-22T04:57:02.467885Z","shell.execute_reply.started":"2022-07-22T04:57:01.257665Z","shell.execute_reply":"2022-07-22T04:57:02.466958Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv('../input/titanic/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:02.469815Z","iopub.execute_input":"2022-07-22T04:57:02.470300Z","iopub.status.idle":"2022-07-22T04:57:02.494420Z","shell.execute_reply.started":"2022-07-22T04:57:02.470270Z","shell.execute_reply":"2022-07-22T04:57:02.493000Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:02.495922Z","iopub.execute_input":"2022-07-22T04:57:02.496429Z","iopub.status.idle":"2022-07-22T04:57:02.524163Z","shell.execute_reply.started":"2022-07-22T04:57:02.496399Z","shell.execute_reply":"2022-07-22T04:57:02.523155Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()          # 전체 Null 값 확인","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:02.526957Z","iopub.execute_input":"2022-07-22T04:57:02.528035Z","iopub.status.idle":"2022-07-22T04:57:02.538370Z","shell.execute_reply.started":"2022-07-22T04:57:02.527992Z","shell.execute_reply":"2022-07-22T04:57:02.537243Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Age, Cabin, Embarked에 Null 값이 있다. 나중에 이것들을 수정해보는 작업을 거칠 예정이다.","metadata":{}},{"cell_type":"markdown","source":"## 2. 얼마나 생존을 했을까?","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(1,2, figsize=(18,8))\ndata['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%', ax=ax[0], shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived', data=data, ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:02.539734Z","iopub.execute_input":"2022-07-22T04:57:02.541379Z","iopub.status.idle":"2022-07-22T04:57:02.870772Z","shell.execute_reply.started":"2022-07-22T04:57:02.541331Z","shell.execute_reply":"2022-07-22T04:57:02.869743Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"많은 탑승객들이 사고로 인해 생존을 하지 못했다.\nTraning Set의 탑승객 891명 중 약 350명(38.4%)만이 생존을 했다.\n데이터로부터 다른 인사이트를 얻고 어떤 유형의 탑승객이 생존했고, 그렇지 못했는지 살펴보기 위해 더 들어가보자.\n\n데이터셋의 여러 feature들을 사용해 생존율을 체크해보자.\nSex(성별), Port of Embarkation(탑승항구), Age(연령) 등이 있다.","metadata":{}},{"cell_type":"markdown","source":"### 2.1. Feature의 유형\n\n1. Categorical Features(카테고리 자료형)\n카테고리형 변수(Categorical Variable)은 그 값으로 두 개 이상의 카테고리를 가지고 각각의 값으로 feature가 카테고리화 될 수 있다.  \n예를 들자면, 성별은 두 개의 카테고리(남성과 여성)를 가진 카테고리형 변수인 셈이다. 순서를 부여할 수 없다는 특징이 있다. 명목 변수(Nomial Variable)라고도 한다.  \n-해당 데이터셋의 카테고리 자료형 예 : 성별(Sex), 탑승 항구(Embarked)  \n성별은 위에서 말한대로 남성 혹은 여성이라는 2개의 카테고리를 가지고 있으며, 탑승항구 역시 P, C, S라는 3가지의 카테고리를 가지기 때문에 카테고리 자료형이라고 할 수 있다.\n\n2. Ordinal Feature(순서 자료형)\n순서형 변수(Ordinal Variable)은 카테고리형 변수와 비슷하지만, 변수 안 각 값들 간 상대적인 순서, 분류를 부여할 수 있다는 점이 다르다.  \nTall, Medium, Short의 값을 가진 높이와 같은 자료형들은 순서형 변수라고 할 수 있다. 이 변수 안에서 우리가 상대적인 분류가 가능하기 때문이다.  \n-해당 데이터셋의 순서 자료형 예 : 좌석 등급(Pclass)  \n좌석 등급의 경우 1등석, 2등석, 3등석과 같이 서수 형태의 숫자로 등급이 구별됐기 때문에 순서 자료형이라고 할 수 있다.\n\n3. Continuous Feature(연속적 자료형)\n어떤 변수가 특정 두 지점, 혹은 최댓값과 최솟값 사이에 어떤 값이든 가질 수 있다면 그 변수는 연속형이다. 셀 수 없는 자료형으로도 불린다.  \n키, 몸무게 등과 같은 것들이 대표적인 연속적 자료형이라고 할 수 있다.  \n-해당 데이터셋의 연속 자료형 예 : 나이(Age)  \n나이는 Mlle, Maam, Mr, Mrs, Master 등 나이를 대강 유추할 수 있어서 순서 자료형이거나 카테고리형 데이터라고 할 수 있을 거 같지만  \n여기에서는 Null Data 부분을 제외한 나머지 데이터상으로는 자신의 나이를 기록하였기 때문에 연속적 자료형라고 할 수 있다.\n\n나이의 경우는 추가 설명을 더 하자면, 나이는 데이터를 수집하는 과정에서 데이터의 종류가 다르게 취급될 수 있다는 것이다.\n해당 데이터셋과 다르게 '영아', '청소년', '청년', '장년' 등으로 기록하였다면 카테고리 자료형이라고 할 수 있고, \n10대, 20대 등으로 구별하였다면 순서형 데이터가 될 수도 있다.\n","metadata":{}},{"cell_type":"markdown","source":"## 3. Feature 분석하기","metadata":{}},{"cell_type":"markdown","source":"### 3.1. Sex → Categorical Feature","metadata":{}},{"cell_type":"code","source":"data.groupby(['Sex', 'Survived'])['Survived'].count().to_frame()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:02.872174Z","iopub.execute_input":"2022-07-22T04:57:02.875016Z","iopub.status.idle":"2022-07-22T04:57:02.889758Z","shell.execute_reply.started":"2022-07-22T04:57:02.874958Z","shell.execute_reply":"2022-07-22T04:57:02.888589Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,2, figsize=(15,6))\n\ndata[['Sex', 'Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived VS Sex')\n\nsns.countplot('Sex', hue='Survived', data=data, ax=ax[1])\nax[1].set_title('Sex : Survived VS Dead')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:02.891007Z","iopub.execute_input":"2022-07-22T04:57:02.891465Z","iopub.status.idle":"2022-07-22T04:57:03.181750Z","shell.execute_reply.started":"2022-07-22T04:57:02.891440Z","shell.execute_reply":"2022-07-22T04:57:03.180497Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"흥미로운 결과이다. 남자 탑승객의 수가 여자 탑승객의 수보다 훨씬 많은 걸 우선 확인할 수 있다. 하지만 여자 생존객 탑승객 수가 남자 생존 탑승객의 수보다 거의 2배 이상으로 많다.\n여성의 생존율은 75% 정도인데 반해 남자의 생존율은 20%에도 못 미친다. 때문에 성별은 모델링에 매우 중요한 Feature가 될 수 있다.","metadata":{}},{"cell_type":"markdown","source":"### 3.2. Pclass → Ordinal Feature","metadata":{}},{"cell_type":"code","source":"pd.crosstab(data.Pclass, data.Survived, margins = True).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:03.183538Z","iopub.execute_input":"2022-07-22T04:57:03.184122Z","iopub.status.idle":"2022-07-22T04:57:03.305450Z","shell.execute_reply.started":"2022-07-22T04:57:03.184092Z","shell.execute_reply":"2022-07-22T04:57:03.304478Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"f,ax = plt.subplots(1,2, figsize=(15,6))\n\ndata['Pclass'].value_counts().plot.bar(color=['#CD7F32', '#FFDF00', '#D3D3DE'], ax=ax[0])\nax[0].set_title('Number of Passengers by Pclass')\nax[0].set_ylabel('')\n\nsns.countplot('Pclass', hue='Survived', data=data, ax=ax[1])\nax[1].set_title('Pclass : Survived VS Dead')","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:03.306809Z","iopub.execute_input":"2022-07-22T04:57:03.307045Z","iopub.status.idle":"2022-07-22T04:57:03.597313Z","shell.execute_reply.started":"2022-07-22T04:57:03.307022Z","shell.execute_reply":"2022-07-22T04:57:03.595827Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"돈으로 모든 것을 살 수 없다고 흔히 말하지만 Pclass 1의 생존자가 구조 시에 매우 우선 순위에 있었던 거 같다.\nPclass 3의 탑승객 수가 훨씬 많았지만, 생존자 비율은 25% 정도에 불과해 매우 낮다.\nPclass 1의 생존율은 63%, Pclass 2의 생존율은 48% 정도이다. 결국 돈과 지위가 중요한 요소로 작용한 것으로 보인다. ~~1등석만 살아남는 더러운 세상~~\n\n이번에는 Pclass와 Sex를 함께 두고 생존율을 체크해보는 과정을 보자.","metadata":{}},{"cell_type":"code","source":"pd.crosstab([data.Sex, data.Survived], data.Pclass, margins = True).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:03.602275Z","iopub.execute_input":"2022-07-22T04:57:03.602947Z","iopub.status.idle":"2022-07-22T04:57:03.658801Z","shell.execute_reply.started":"2022-07-22T04:57:03.602905Z","shell.execute_reply":"2022-07-22T04:57:03.657615Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"sns.factorplot('Pclass', 'Survived', hue='Sex', data=data)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:03.660183Z","iopub.execute_input":"2022-07-22T04:57:03.661127Z","iopub.status.idle":"2022-07-22T04:57:04.054061Z","shell.execute_reply.started":"2022-07-22T04:57:03.661094Z","shell.execute_reply":"2022-07-22T04:57:04.053087Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"카테고리 자료형(Categorical Value)을 쉽게 보기 위해 Factor Plot을 사용했다. \nCrossTab과 FactorPlot으로 미루어 보아 Pclass 1 여성 탑승객의 생존율이 95 ~ 96%로 사망자가 겨우 3명만이 있었다.\n그리고 Pclass와 무관하게, 여성이 구조에 있어서 우선 순위에 있었다. 남성의 경우 Pclass 1라도 생존률은 매우 낮았다.  ~~남녀 차별(?)~~\nPclass 역시 중요한 Feature로 여겨진다. 다른 Feature를 또 분석해보자","metadata":{}},{"cell_type":"markdown","source":"### 3.3. Age → Continuous Feature","metadata":{}},{"cell_type":"code","source":"print('Oldest Passenger was of: ', data['Age'].max(),'Years')\nprint('Youngest Passeger was of: ', data['Age'].min(),'Years')\nprint('Average Age on the ship: ', data['Age'].mean(), 'Years')","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:04.055331Z","iopub.execute_input":"2022-07-22T04:57:04.055591Z","iopub.status.idle":"2022-07-22T04:57:04.063986Z","shell.execute_reply.started":"2022-07-22T04:57:04.055565Z","shell.execute_reply":"2022-07-22T04:57:04.062450Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,2, figsize=(15,6))\n\nsns.violinplot('Pclass', 'Age', hue='Survived', data=data, split=True, ax=ax[0])\nax[0].set_title('Pclass and Age VS Survived')\nax[0].set_yticks(range(0,110,10))\n\nsns.violinplot('Sex', 'Age', hue='Survived', data=data, split=True, ax=ax[1])\nax[1].set_title('Sex and Age VS Survived')\nax[1].set_yticks(range(0,110,10))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:04.065509Z","iopub.execute_input":"2022-07-22T04:57:04.066131Z","iopub.status.idle":"2022-07-22T04:57:04.474647Z","shell.execute_reply.started":"2022-07-22T04:57:04.066092Z","shell.execute_reply":"2022-07-22T04:57:04.474041Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"#### 관찰결과 : \n1) Pclass 등급이 낮아질수록(1등석에서 3등석으로 갈수록) 어린이의 수 증가. 10세 이하의 탑승객 수는 Pclass 수와 관계 없이 양호하다.\n2) 20 ~ 50세 사이의 Pclass 1 탑승객 생존율이 높은 편이며, 여성의 경우에 더 높다.\n3) 남성의 연령이 증가할수록 생존 확률이 줄어든다.\n\n앞에서 본 것처럼, Age Feature은 177개의 Null값을 가지고 있었다.\n이 값들은 데이터 셋의 평균값으로 대체가 가능하다.\n하지만 사람들의 연령대는 다양하게 분포하기 때문에, 4세 어린에게 29세의 연령을 부여할 수도 있는 노릇이다.\n승객이 어떤 연령대에 속했는지 알 수 있는 방법을 찾아보기 위해 탑승객의 이름을 체크해볼 필요가 있다. Name을 보면 Mr, Mrs와 같은 salutation(인사, 호칭)이 있다.","metadata":{}},{"cell_type":"code","source":"data['Initial'] = 0\ndata['Initial'] = data.Name.str.extract('([A-aZ-z]+)\\.')    # salutation을 추출합니다.","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:04.475766Z","iopub.execute_input":"2022-07-22T04:57:04.476008Z","iopub.status.idle":"2022-07-22T04:57:04.484496Z","shell.execute_reply.started":"2022-07-22T04:57:04.475985Z","shell.execute_reply":"2022-07-22T04:57:04.483687Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"정규표현식 [A-Za-Z]+를 사용했다. 이 정규표현식은 A-Z 또는 a-z 사이의 문자열과 그 뒤에 있는 .(dot)을 찾아낸다.\n이것으로 Name에서 Salutation을 추출했다.","metadata":{}},{"cell_type":"code","source":"pd.crosstab(data.Initial,data.Sex).T.style.background_gradient(cmap='summer_r')\n# 성별에 따른 Initial 체크","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:04.485642Z","iopub.execute_input":"2022-07-22T04:57:04.485964Z","iopub.status.idle":"2022-07-22T04:57:04.532834Z","shell.execute_reply.started":"2022-07-22T04:57:04.485935Z","shell.execute_reply":"2022-07-22T04:57:04.531235Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Miss를 나타내는 Mlle, Mme와 같은 잘못 적힌 Initial 이 있는데, 값들을 Miss 등의 다른 값들로 대체한다. replace() 메서드를 이용해 1대 1 대응을 시켜함수를 활용해서 대체하는 과정이다.","metadata":{}},{"cell_type":"code","source":"data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt', 'Sir','Don'],\n                                  ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:04.534332Z","iopub.execute_input":"2022-07-22T04:57:04.534689Z","iopub.status.idle":"2022-07-22T04:57:04.544691Z","shell.execute_reply.started":"2022-07-22T04:57:04.534654Z","shell.execute_reply":"2022-07-22T04:57:04.543650Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"data.groupby('Initial')['Age'].mean()  # Initial에 따른 평균연령 체크","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:04.546304Z","iopub.execute_input":"2022-07-22T04:57:04.547764Z","iopub.status.idle":"2022-07-22T04:57:04.560272Z","shell.execute_reply.started":"2022-07-22T04:57:04.547652Z","shell.execute_reply":"2022-07-22T04:57:04.559248Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### 3.3.1. 연령 NaN 채우기","metadata":{}},{"cell_type":"code","source":"# 평균의 올림 값들로 NaN 값에 할당\ndata.loc[(data.Age.isnull()) & (data.Initial == 'Mr'), 'Age'] = 33\ndata.loc[(data.Age.isnull()) & (data.Initial == 'Mrs'), 'Age'] = 36\ndata.loc[(data.Age.isnull()) & (data.Initial == 'Master'), 'Age'] = 5\ndata.loc[(data.Age.isnull()) & (data.Initial == 'Miss'), 'Age'] = 22\ndata.loc[(data.Age.isnull()) & (data.Initial == 'Other'), 'Age'] = 46","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:04.561789Z","iopub.execute_input":"2022-07-22T04:57:04.562635Z","iopub.status.idle":"2022-07-22T04:57:04.575900Z","shell.execute_reply.started":"2022-07-22T04:57:04.562594Z","shell.execute_reply":"2022-07-22T04:57:04.575072Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"data.Age.isnull().any()   # Null 값들이 모두 제거","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:04.577185Z","iopub.execute_input":"2022-07-22T04:57:04.577413Z","iopub.status.idle":"2022-07-22T04:57:04.583526Z","shell.execute_reply.started":"2022-07-22T04:57:04.577391Z","shell.execute_reply":"2022-07-22T04:57:04.582867Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,2, figsize=(15,6))\n\ndata[data['Survived']==0].Age.plot.hist(ax=ax[0], bins=20, edgecolor='black', color='red')\nax[0].set_title('Survived = 0')\nx1 = list(range(0,85,5))\nax[0].set_xticks(x1)\n\ndata[data['Survived']==1].Age.plot.hist(ax=ax[1], bins=20, edgecolor='black', color='green')\nax[1].set_title('Survived = 1')\nx2 = list(range(0,85,5))\nax[1].set_xticks(x2)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:04.584666Z","iopub.execute_input":"2022-07-22T04:57:04.585103Z","iopub.status.idle":"2022-07-22T04:57:04.939487Z","shell.execute_reply.started":"2022-07-22T04:57:04.585076Z","shell.execute_reply":"2022-07-22T04:57:04.938591Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"#### 관찰결과:\n\n1) 5세 이하의 아이들이 많이 생존했다. 사고 당시 여성과 아이를 우선으로 구조하려 했다.  \n2) 가장 고연령 탑승객도 생존했다.(80세)  \n3) 가장 많은 수의 사망자가 있는 연령 그룹은 30 ~ 40세이다.  ","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Pclass', 'Survived', col = 'Initial', data=data)\nplt.show","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:04.940615Z","iopub.execute_input":"2022-07-22T04:57:04.941323Z","iopub.status.idle":"2022-07-22T04:57:06.001436Z","shell.execute_reply.started":"2022-07-22T04:57:04.941290Z","shell.execute_reply":"2022-07-22T04:57:06.000426Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"각 클래스에 관계없이 여성과 아이가 우선적으로 구조되었다는 것이 더 명확해졌다.","metadata":{}},{"cell_type":"markdown","source":"### 3.4. Embarked → Categorical Value","metadata":{}},{"cell_type":"code","source":"pd.crosstab([data.Embarked, data.Pclass], [data.Sex, data.Survived], margins =True).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:06.002928Z","iopub.execute_input":"2022-07-22T04:57:06.003603Z","iopub.status.idle":"2022-07-22T04:57:06.085848Z","shell.execute_reply.started":"2022-07-22T04:57:06.003563Z","shell.execute_reply":"2022-07-22T04:57:06.084763Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"#### 3.4.1 탑승 항구에 따른 생존확률","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Embarked', 'Survived', data=data)\nfig = plt.gcf()\nfig.set_size_inches(5,3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:06.087563Z","iopub.execute_input":"2022-07-22T04:57:06.087995Z","iopub.status.idle":"2022-07-22T04:57:06.406815Z","shell.execute_reply.started":"2022-07-22T04:57:06.087964Z","shell.execute_reply":"2022-07-22T04:57:06.405781Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"C 항구에서 탑승한 사람들의 생존율이 가장 높은 55% 정도임을 알 수 있고, S 항구에소 탑승한 사람들이 가장 생존율이 낮았다.","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(2, 2, figsize=(15, 12))\n\nsns.countplot('Embarked', data=data, ax=ax[0,0])\nax[0,0].set_title('No. of Passengers Boarded')\n\nsns.countplot('Embarked', hue='Sex', data=data, ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\n\nsns.countplot('Embarked', hue='Survived', data=data, ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\n\nsns.countplot('Embarked', hue = 'Pclass', data=data, ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\n\nplt.subplots_adjust(wspace=0.2, hspace=0.2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:06.408284Z","iopub.execute_input":"2022-07-22T04:57:06.408830Z","iopub.status.idle":"2022-07-22T04:57:06.931493Z","shell.execute_reply.started":"2022-07-22T04:57:06.408796Z","shell.execute_reply":"2022-07-22T04:57:06.930669Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"#### 관찰결과 : \n1) S에서 가장 많은 승객이 탑승하였는데, 거기에서의 탑승객들은 대부분 Pclass 3였다.  \n2) C에서 탑승한 승객들은 운이 좋게 Pclass 1과 2에 있었기 때문에 생존율이 높은 것으로 밝혀졌다.  \n3) S항구에서 다수의 부유한 사람들이 탑승한 거 같다. 그러나 같은 항구에 탑승했던 Pclass3의 승객이 압도적으로 많아 S항구에서 탑승한 사람들의 생존율이 매우 낮았다.  \n4) Q항구에서 탑승한 승객의 95% 가량이 Pclass 3이다.","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Pclass', 'Survived', hue = 'Sex', col = 'Embarked', data=data)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:06.933190Z","iopub.execute_input":"2022-07-22T04:57:06.933766Z","iopub.status.idle":"2022-07-22T04:57:08.095615Z","shell.execute_reply.started":"2022-07-22T04:57:06.933732Z","shell.execute_reply":"2022-07-22T04:57:08.094345Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"#### 관찰결과 : \n\n1) Pclass 1과 Pclass2 여성의 생존률은 Pclass와 관계 없이 거의 1에 근접했다.\n\n2) S 항구에서 탑승한 Pclass 3의 탑승객은 매우 운이 없는 것 같다. 남성과 여성의 생존률이 모두 낮다. (역시나 돈 많으면 장땡인 거 같다.)\n\n3) Q 항구에서 탑승한 남성이 제일 불운해 보인다. 그들 대부분이 Pclass 3 탑승객이기 때문이다.","metadata":{}},{"cell_type":"markdown","source":"#### 3.4.2. Embarked 의 NaN 채우기\n\n대부분의 탑승객이 S에서 탑승했기 때문에 S로 채워주도록 하겠다.","metadata":{}},{"cell_type":"code","source":"data['Embarked'].fillna('S', inplace = True)\ndata['Embarked'].isnull().any()     # Nan 모두 제거","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:08.097502Z","iopub.execute_input":"2022-07-22T04:57:08.097965Z","iopub.status.idle":"2022-07-22T04:57:08.109194Z","shell.execute_reply.started":"2022-07-22T04:57:08.097926Z","shell.execute_reply":"2022-07-22T04:57:08.107972Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### 3.5. SibSp → Discrete Feature","metadata":{}},{"cell_type":"markdown","source":"이 Feature는 탑승객이 혼자인지 아니면 가족과 함께 탔는지를 나타낸다.  \nSibling : 형제자매, 의붓형제자매\nSpouse : 배우자","metadata":{}},{"cell_type":"code","source":"pd.crosstab([data.SibSp], data.Survived).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:08.110632Z","iopub.execute_input":"2022-07-22T04:57:08.110937Z","iopub.status.idle":"2022-07-22T04:57:08.134890Z","shell.execute_reply.started":"2022-07-22T04:57:08.110912Z","shell.execute_reply":"2022-07-22T04:57:08.133885Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(20,8))\n\nsns.barplot('SibSp','Survived',data=data, ax=ax[0])\nax[0].set_title('SibSp vs Survived')\n\nsns.pointplot('SibSp','Survived',data=data, ax=ax[1])\nax[1].set_title('SibSp vs Survived')\n\nplt.close(2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:08.141055Z","iopub.execute_input":"2022-07-22T04:57:08.141341Z","iopub.status.idle":"2022-07-22T04:57:08.685050Z","shell.execute_reply.started":"2022-07-22T04:57:08.141318Z","shell.execute_reply":"2022-07-22T04:57:08.683629Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(data.Parch, data.Pclass).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:08.686475Z","iopub.execute_input":"2022-07-22T04:57:08.686888Z","iopub.status.idle":"2022-07-22T04:57:08.712624Z","shell.execute_reply.started":"2022-07-22T04:57:08.686855Z","shell.execute_reply":"2022-07-22T04:57:08.711355Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"#### 관찰결과 : \n\nbarplot을 통해 봤을 때, 승객이 혼자 탑승했을 때 생존률이 34.5% 정도로 추정된다.  \n만약, 가족과 함께 탔다면 내가 생존하기 전에 가족을 살리려고 할 것이다.  \n하지만 놀랍게도 가족의 수가 5-8명인 경우에는 생존률이 0%인데, Pclass의 때문일 가능성이 있을지도 모른다.  \n실제로 놓고 보니 그 원인이 Pclass였다. crosstab을 보면 SibSp > 3 인 경우 모두 Pclass 3에 속해 있는 것을 확인할 수 있다.  \n다시 말해, Pclass 3의 3명 초과 가족들은 모두 사망한 것이 분명하다.  ","metadata":{}},{"cell_type":"markdown","source":"#### 3.5.1 Parch","metadata":{}},{"cell_type":"code","source":"pd.crosstab(data.Parch, data.Pclass).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:08.713938Z","iopub.execute_input":"2022-07-22T04:57:08.714247Z","iopub.status.idle":"2022-07-22T04:57:08.744630Z","shell.execute_reply.started":"2022-07-22T04:57:08.714218Z","shell.execute_reply":"2022-07-22T04:57:08.743636Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"crosstab을 통해 또 구성원이 많은 가족들은 대부분 Pclass 3에 속함을 알 수 있다.","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize=(15,6))\n\nsns.barplot('Parch', 'Survived', data=data, ax=ax[0])\nax[0].set_title('Parch vs Survived')\n  \nsns.pointplot('Parch', 'Survived', data=data, ax=ax[1])\nax[1].set_title('Parch vs Survived')\n# factorplot에서 pointplot으로 바꿔준다. 그러면 오른쪽에 그래프가 정상적으로 나타난다.\n\nplt.close(2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:08.746128Z","iopub.execute_input":"2022-07-22T04:57:08.747081Z","iopub.status.idle":"2022-07-22T04:57:09.252155Z","shell.execute_reply.started":"2022-07-22T04:57:08.747052Z","shell.execute_reply":"2022-07-22T04:57:09.250768Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"#### 관찰결과 :\n\n비슷한 결과가 나왔다. 부모님, 아이와 함꼐 탑승한 승객들의 생존 확률은 높았다.  \n하지만 그 수가 증가할 수록 생존률은 감소했다.\n\n1-3명의 부모님, 아이와 탑승한 승객의 생존률이 좋았다.  \n혼자 탑승하는 경우, 생존하기가 어려우며, 가족이 4명이상 탑승한 경우에도 생존률은 감소했다.","metadata":{}},{"cell_type":"markdown","source":"### 3.6. Fare → Continuous Feature","metadata":{}},{"cell_type":"code","source":"print('Highest Fare was: ', data['Fare'].max())\nprint('Lowest Fare was: ', data['Fare'].min())\nprint('Average Fare was: ', data['Fare'].mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:09.253235Z","iopub.execute_input":"2022-07-22T04:57:09.253489Z","iopub.status.idle":"2022-07-22T04:57:09.260355Z","shell.execute_reply.started":"2022-07-22T04:57:09.253466Z","shell.execute_reply":"2022-07-22T04:57:09.259587Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,3, figsize=(15,5))\n\n# distplot : 분포도(distribution)를 나타낸 히스토그램 그래프. seaborn을 활용하여 그래프를 나타낸다.\nsns.distplot(data[data['Pclass']==1].Fare, ax=ax[0])\nax[0].set_title('Fare in Pclass 1')\n\nsns.distplot(data[data['Pclass']==2].Fare, ax=ax[1])\nax[1].set_title('Fare in Pclass 2')\n\nsns.distplot(data[data['Pclass']==3].Fare, ax=ax[2])\nax[2].set_title('Fare in Pclass 3')","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:09.261525Z","iopub.execute_input":"2022-07-22T04:57:09.262249Z","iopub.status.idle":"2022-07-22T04:57:09.877454Z","shell.execute_reply.started":"2022-07-22T04:57:09.262184Z","shell.execute_reply":"2022-07-22T04:57:09.876297Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"히스토그램은 수치형 데이터 분포를 정확하게 표현해 주는 그래픽이다. 위의 그래프는 Pclass별 요금 분포를 나타낸 히스토그램이다.\n\nPclass 1 탑승객의 경우 요금 분포가 넓게 퍼져있었다. 그리고 Pclass의 등급이 낮아질 때마다 분포는 좁아지는 것을 확인할 수 있다. 이 변수는 연속형이기 때문에, 우리는 binning을 통해 이산형 값들로 변환해줄 것이다.","metadata":{}},{"cell_type":"markdown","source":"## 4. 모든 Feature들에 대한 관찰 요약\n\n1) sex : 여성의 생존확률이 남성에 비해 높았다.\n\n2) Pclass : 1st 클래스 탑승객의 생존율이 높은 경향을 보였다. Pclass가 안 좋을수록 생존율이 낮았다. 여성의 경우 Pclass 1 탑승객의 생존율은 거의 1이었고, Pclass 2의 경우에도 생존율이 높은 편이었다. 결국 생존에는 돈이 중요했다.\n\n3) Age : 5~10세보다 적은 어린이들의 생존확률이 높았다. 15-35세의 탑승객들은 많이 사망했다.\n\n4) Embarked : 흥미로운 Feature가 나왔다. 다수의 Pclass 1 탑승객이 S에서 제일 많았지만, C에서 탑승한 승객의 생존이 더 높았다. Q에서 탑승한 승객은 거의 다 Pclass 3 에 속하여 생존율이 낮은 편에 속한다.\n\n5) Parch + SibSp : 1-2명의 형제자매, 1-3명의 가족, 자녀와 함께 탑승한 경우가 혼자 탑승 또는 많은 수의 가족과 함께 탑승한 경우보다 훨씬 생존율이 높았다.","metadata":{}},{"cell_type":"markdown","source":"## 5. 특성들 사이의 상관관계표(Correlation Between The Feature)","metadata":{}},{"cell_type":"markdown","source":"heatmap : 열을 뜻하는 heat와 지도를 뜻하는 map의 합성어. 색상으로 표현할 수 있는 다양한 정보를 일정한 이미지 위에 열분포 형태의 비쥬얼한 그래픽으로 출력하는 것이 특징이다.\n","metadata":{}},{"cell_type":"code","source":"sns.heatmap(data.corr(), annot=True, cmap='RdYlGn', linewidths=0.2)\n# data.corr() : 상관관계 행렬을 나타냄\n\nfig = plt.gcf()\nfig.set_size_inches(10,8)\nplt.show","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:09.878520Z","iopub.execute_input":"2022-07-22T04:57:09.879447Z","iopub.status.idle":"2022-07-22T04:57:10.316989Z","shell.execute_reply.started":"2022-07-22T04:57:09.879417Z","shell.execute_reply":"2022-07-22T04:57:10.316113Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"### 5.1. Heatmap의 해석\n\n먼저 알아야 할 것은, 숫자 데이터가 아닌 문자열 데이터의 상관관계를 구할 수 없다는 것이다.\nplot을 이해하기 전에 상관관계가 무엇인지 보자\n\n1) 양의 상관관계(Positive Correlation)\nFeature A가 증가할 때 Feature B도 증가한다면, 두 Feature 간에는 양의 상관관계가 성립한다. 1은 완전 양의 상관관계를 의미한다.\n\n2) 음의 상관관계(Negative Correlation)\nFeature A가 증가함에 따라 Feature B가 감소한다면, 두 Feature 간에는 음의 상관관계가 성립한다. -1은 완전 의 상관관계를 의미한다.\n\n두 Feature가 상당히 높은, 혹은 완전한 양의 상관관계를 가지고 있다고 하면, 한 feature 값이 증가할 때 다른 feature의 값도 증가한다. 이것은 두 feature가 매우 비슷한 정보를 가지고 있으며, 그 정보간의 분산이 거의 없다는 것을 의미한다. 이를 다중공선성(MultiColinearity)이라고 한다.\n이 변수들이 불필요할 때(redundant), 우리는 그 변수를 둘 다 사용해야 하는가?\n모델을 만들거나 학습시킬 때, 학습시간을 줄이는 등 다른 이점을 위해 불필요한 feature는 제거되도록 해야 한다.\n위의 Heatmap을 보았을 때, feature들 간의 상관관계는 그렇게 높아 보이지 않는다.\n가장 높은 상관관계를 지닌 두 변수는 SibSp와 Parch로 상관계수는 0.41이다. 그렇기 때문에 모든 feature를 사용해보도록 하겠다.","metadata":{}},{"cell_type":"markdown","source":"# Part2 : 자료형 엔지니어링과 데이터 정화(Feature Engineering and Data Cleansing)","metadata":{}},{"cell_type":"markdown","source":"Feature Engineering이 무엇일까?\nfeature들이 있는 dataset이 주어졌을 때, 모든 feature가 중요하지는 않다.\n제거되어야 할 불필요한 Feature가 있을 수도 있고\n다른 feature의 관찰 및 정보 추출을 통해 새로운 feature를 만들 수도 있다.\n\nName으로부터 Initial 을 만들어낸 것도 한 예시이다. 새로운 feature를 만들거나 제거해야할 Feature가 있는지 살펴보자.\n그리고 예측 모델에 적합한 형태로 feature들로 변환해보자.","metadata":{}},{"cell_type":"markdown","source":"## 1. Age_band","metadata":{}},{"cell_type":"markdown","source":"먼저 언급했듯이, Age는 연속적 자료형이다. 하지만, 연속적 자료형의 경우 머신러닝모델에 있어 문제가 하나 있습니다.\n\n가령, 운동선수들을 성별로 그룹을 나눈다고 할 때 우리는 쉽게 남성, 여셩으로 나눌 수 있다.\n\n연령으로 그룹을 나눈다고 할때, 어떻게 나눌수 있을까? 만약 30명의 사람에 30개의 연령값이 있다고 하면 구별을 어떻게 하라는 것인가? 연령층을 구별해 주는 등 기준을 잡지 못하면 구별할 수가 없을 것이다.\n\n우리는 연속적인 값을 카테고리 값으로 Binning(구간화)이나 Normalization(정규화)을 통해 변환해야합니다. 이번에는 binning을 통해 연령에 하나의 값을 할당하겠습니다.\n\n최대 연령이 80세이기 떄문에, 0부터 80세까지의 연령을 5개의 bin(구간)으로 나눈다. 80/5 = 16 이기 때문에, 구간 하나의 사이즈는 16입니다.","metadata":{}},{"cell_type":"code","source":"data['Age_band'] = 0\n\ndata.loc[data['Age'] <= 16, 'Age_band'] = 0\ndata.loc[(data['Age'] > 16) & (data['Age'] <=32), 'Age_band'] = 1\ndata.loc[(data['Age'] > 32) & (data['Age'] <=48), 'Age_band'] = 2\ndata.loc[(data['Age'] > 48) & (data['Age'] <=64), 'Age_band'] = 3\ndata.loc[data['Age'] > 64, 'Age_band'] = 4\n# 0세에서 16세 이하 : 0번 구간\n# 16세 초과 32세 이하 : 1번 구간\n# 32세 초과 48세 이하 : 2번 구간\n# 48세 초과 64세 이하 : 3번 구간\n# 64세 초과 : 4번 구간\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:10.318225Z","iopub.execute_input":"2022-07-22T04:57:10.318562Z","iopub.status.idle":"2022-07-22T04:57:10.347215Z","shell.execute_reply.started":"2022-07-22T04:57:10.318536Z","shell.execute_reply":"2022-07-22T04:57:10.345957Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"data['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer')\n# 각 연령구간의 탑승객 수를 체크하여 내림차순으로 정","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:10.348910Z","iopub.execute_input":"2022-07-22T04:57:10.349296Z","iopub.status.idle":"2022-07-22T04:57:10.365089Z","shell.execute_reply.started":"2022-07-22T04:57:10.349267Z","shell.execute_reply":"2022-07-22T04:57:10.363991Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"sns.factorplot('Age_band', 'Survived', data=data, col= 'Pclass')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:10.366612Z","iopub.execute_input":"2022-07-22T04:57:10.367030Z","iopub.status.idle":"2022-07-22T04:57:11.549107Z","shell.execute_reply.started":"2022-07-22T04:57:10.366991Z","shell.execute_reply":"2022-07-22T04:57:11.547771Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"모든 Pclass가 공통적으로 연령이 증가할수록 생존율이 낮아짐을 알 수 있다.","metadata":{}},{"cell_type":"markdown","source":"## 2. Family Size와 Alone","metadata":{}},{"cell_type":"markdown","source":"이번에는 \"Family Size\"와 \"Alone\" Feature를 만들어 분석해보도록 하자.\n이 Feature들은 Parch와 SibSp를 요약한 것이다. 가족의 수와 생존율의 관계를 체크하기 위한 통합된 데이터를 얻을 수 있다.\nAlone은 승객이 혼자인지 아닌지를 나타낸다.","metadata":{}},{"cell_type":"code","source":"data['Family_Size'] = 0\ndata['Family_Size'] = data['Parch'] + data['SibSp']   # Family_Size의 통합된 데이터\ndata['Alone'] = 0\ndata.loc[data.Family_Size == 0, 'Alone'] = 1      # Alone\n\nf, ax = plt.subplots(1,2, figsize=(15,5))\n\nsns.pointplot('Family_Size', 'Survived', data=data, ax=ax[0])\nax[0].set_title('Family_Size VS Survived')\nsns.pointplot('Alone', 'Survived', data=data, ax=ax[1])\nax[1].set_title('Alone VS Survived')\n\nplt.close(2)\nplt.close(3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:11.550612Z","iopub.execute_input":"2022-07-22T04:57:11.550943Z","iopub.status.idle":"2022-07-22T04:57:12.165936Z","shell.execute_reply.started":"2022-07-22T04:57:11.550917Z","shell.execute_reply":"2022-07-22T04:57:12.164854Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"Family_Size = 0 은 탑승객이 혼자임을 의미하는데, 혼자일 때 생존률은 매우 낮은 것을 확인할 수 있다. 가족 수가 4명 이상일 때도 생존률은 감소한다. 때문에 모델링에 중요한 Feature인 것 같다. 조금 더 분석해보겠습니다.","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Age_band', 'Survived', data=data, col = 'Pclass')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:12.167044Z","iopub.execute_input":"2022-07-22T04:57:12.167277Z","iopub.status.idle":"2022-07-22T04:57:12.979478Z","shell.execute_reply.started":"2022-07-22T04:57:12.167255Z","shell.execute_reply":"2022-07-22T04:57:12.978532Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"Pclass와 무관하게 혼자 탑승한 경우는 위험할 확률이 높다. 하지만 예외적으로 Pclass 3 여성 탑승객의 생존률은 가족과 함께 탑승하지 않은 경우보다 오히려 높기 때문에 분석 시 .","metadata":{}},{"cell_type":"markdown","source":"## 3. Fare_Range","metadata":{}},{"cell_type":"markdown","source":"Fare는 연속적 자료형이기 때문에, 이것을 서수형 값(Ordinal value)로 변환하도록 하겠다. 이 작업에 pandas.qcut을 사용할 것이다.\n\nqcut은 우리가 입력한 구간의 수(bin)에 따라 데이터 값을 분할해 주는 역할을 한다. 가령 우리가 5개 구간을 입력하면, 5개의 구간으로 데이터 수를 균일하게 분할하게 된다.","metadata":{}},{"cell_type":"code","source":"data['Fare_Range'] = pd.qcut(data['Fare'], 4)\ndata.groupby(['Fare_Range'])['Survived'].mean().to_frame()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:12.980852Z","iopub.execute_input":"2022-07-22T04:57:12.982095Z","iopub.status.idle":"2022-07-22T04:57:13.004432Z","shell.execute_reply.started":"2022-07-22T04:57:12.982047Z","shell.execute_reply":"2022-07-22T04:57:13.003749Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"위에서 언급했던 것처럼, Fare_Range가 증가할수록 생존율 역시 증가하게 된다.\n\n하지만 우리는 Fare_Range를 그대로 사용하지 않고, Age_Band에서 했던 방법과 동일하게 하나의 값으로 변환해서 구분짓도록 해야 한다.","metadata":{}},{"cell_type":"code","source":"data['Fare_cat'] = 0\ndata.loc[data['Fare'] <= 7.91, 'Fare_cat'] = 0\ndata.loc[(data['Fare'] > 7.91) & (data['Fare']<=14.454), 'Fare_cat'] = 1\ndata.loc[(data['Fare'] > 14.454) & (data['Fare']<=31), 'Fare_cat'] = 2\ndata.loc[(data['Fare'] > 31) & (data['Fare']<=513), 'Fare_cat'] = 3","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:13.005498Z","iopub.execute_input":"2022-07-22T04:57:13.006365Z","iopub.status.idle":"2022-07-22T04:57:13.016644Z","shell.execute_reply.started":"2022-07-22T04:57:13.006335Z","shell.execute_reply":"2022-07-22T04:57:13.015881Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"sns.factorplot('Fare_cat', 'Survived', data=data, hue='Sex')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:13.020462Z","iopub.execute_input":"2022-07-22T04:57:13.021204Z","iopub.status.idle":"2022-07-22T04:57:13.596267Z","shell.execute_reply.started":"2022-07-22T04:57:13.021163Z","shell.execute_reply":"2022-07-22T04:57:13.595073Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"Fare_cat이 증가할수록 생존율이 증가하게 된다. 이것도 Sex와 함께 모델링에 중요한 Feature가 될 수 있을 것으로 예상된다.","metadata":{}},{"cell_type":"markdown","source":"## 4. 문자열 값을 숫자형으로 변환하기\n\n문자열 값을 머신러닝 모델이 사용할 수 없기 때문에, Sex, Embarked 등의 feature를 숫자값으로 변환해줘야 한다.","metadata":{}},{"cell_type":"code","source":"data['Sex'].replace(['male','female'], [0,1], inplace=True)\ndata['Embarked'].replace(['S','C','Q'], [0,1,2], inplace=True)\ndata['Initial'].replace(['Mr', 'Mrs', 'Miss', 'Master', 'Other'], [0,1,2,3,4], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:13.597946Z","iopub.execute_input":"2022-07-22T04:57:13.598300Z","iopub.status.idle":"2022-07-22T04:57:13.611452Z","shell.execute_reply.started":"2022-07-22T04:57:13.598262Z","shell.execute_reply":"2022-07-22T04:57:13.610474Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"#### 필요하지 않은 Feature Drop 하기\n\nName : 카테고리 자료형으로 변환할 수 없으므로 필요없다.  \nAge : Age_band로 구간화 작업을 완료했으므로 필요없다.  \nTicket : 이름과 마찬가지로 카테고리화 될 수 없는 무작위 문자열이므로 필요없다.  \nFare : Fare_Cat으로 구간화 작업을 완료했으므로 필요없다.  \nFare_Range : 마찬가지로 Fare_Cat이 있기 때문에 필요 없다.  \nCabin : NaN 값이 너무 많고, 많은 승객에 따라 cabin 값이 많다. 그렇기 때문에 필요없다.  \nPassengerId : 카테고리화 될 수 없기 때문에 필요없다.  ","metadata":{}},{"cell_type":"code","source":"data.drop(['Name', 'Age', 'Ticket', 'Fare', 'Cabin', 'Fare_Range', 'PassengerId'], axis=1, inplace=True)\n\nsns.heatmap(data.corr(), annot=True, cmap='RdYlGn', linewidths=0.2, annot_kws={'size' : 20})\nfig = plt.gcf()\nfig.set_size_inches(15, 15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:13.613108Z","iopub.execute_input":"2022-07-22T04:57:13.613741Z","iopub.status.idle":"2022-07-22T04:57:14.342117Z","shell.execute_reply.started":"2022-07-22T04:57:13.613661Z","shell.execute_reply":"2022-07-22T04:57:14.341182Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"위의 상관관계도에서 몇몇 양의 상관관계를 가진 feature들을 볼 수 있다.\nSibSp, Family_Size와 Parch 가 양의 상관관계를 가지고, Alone과 Family_Size는 음의 상관관계를 가지고 있다.","metadata":{}},{"cell_type":"markdown","source":"# Part3 : 예측 모델링(Predictive Modeling)","metadata":{}},{"cell_type":"markdown","source":"EDA를 통해 인사이트를 얻었지만, 그것만으로는 승객의 생존 여부를 정확히 예측할 수 없다. 우리는 몇몇 훌륭한 분류 알고리즘을 사용하여 승객의 생존여부를 예측할 것이다. 아래의 알고리즘을 모델을 만드는데 사용할 것이다.\n\n1) Logistic Regression  \n2) Support Vector Machines(Linear and radial)  \n3) Random Forest  \n4) K-Nearest Neighbors  \n5) Naive Bayes  \n6) Decision Tree","metadata":{}},{"cell_type":"markdown","source":"## 1. 필요한 머신러닝 패키지 불러들이기","metadata":{}},{"cell_type":"markdown","source":"아래에 있는 from A import B 구문을 통해 머신러닝 패키지를 불러들인다. 이 때 우리는 사이킷런을 활용할 것이다.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:14.343833Z","iopub.execute_input":"2022-07-22T04:57:14.344099Z","iopub.status.idle":"2022-07-22T04:57:14.678214Z","shell.execute_reply.started":"2022-07-22T04:57:14.344072Z","shell.execute_reply":"2022-07-22T04:57:14.676754Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"## 2. 훈련 세트와 테스트 세트의 분할","metadata":{}},{"cell_type":"markdown","source":"train_test_split을 활용하여 훈련 세트와 테스트 세트를 준비한다. 1대 1로 대응되는 데이터 묶음을 무작위로 섞는 작업을 하게 될 것이다.","metadata":{}},{"cell_type":"code","source":"train, test = train_test_split(data, test_size = 0.3, random_state = 0, stratify=data['Survived'])\ntrain_X = train[train.columns[1:]]\ntrain_Y = train[train.columns[:1]]\ntest_X = test[test.columns[1:]]\ntest_Y = test[test.columns[:1]]\nX = data[data.columns[1:]]\nY = data['Survived']","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:14.679724Z","iopub.execute_input":"2022-07-22T04:57:14.680072Z","iopub.status.idle":"2022-07-22T04:57:14.694681Z","shell.execute_reply.started":"2022-07-22T04:57:14.680026Z","shell.execute_reply":"2022-07-22T04:57:14.693518Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"## 3. 머신러닝 모델을 통한 학습 및 결과 확인","metadata":{}},{"cell_type":"markdown","source":"### 3.1. Radial Support Vector Machines(rbf-SVM)","metadata":{}},{"cell_type":"code","source":"model = svm.SVC(kernel = 'rbf', C=1, gamma = 0.1)\nmodel.fit(train_X, train_Y)\nprediction1 = model.predict(test_X)\nprint('Accuracy for rbf SVM is ', metrics.accuracy_score(prediction1, test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:14.696779Z","iopub.execute_input":"2022-07-22T04:57:14.697326Z","iopub.status.idle":"2022-07-22T04:57:14.747996Z","shell.execute_reply.started":"2022-07-22T04:57:14.697295Z","shell.execute_reply":"2022-07-22T04:57:14.747003Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"### 3.2. Linear Support Vector Machines(linear-SVM)","metadata":{}},{"cell_type":"code","source":"model = svm.SVC(kernel = 'linear', C=0.1, gamma=0.1)\nmodel.fit(train_X, train_Y)\nprediction2 = model.predict(test_X)\nprint('Accuracy for linear SVM is ', metrics.accuracy_score(prediction2, test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:14.749581Z","iopub.execute_input":"2022-07-22T04:57:14.749986Z","iopub.status.idle":"2022-07-22T04:57:14.788232Z","shell.execute_reply.started":"2022-07-22T04:57:14.749954Z","shell.execute_reply":"2022-07-22T04:57:14.787367Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"### 3.3. Logistic Regression","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(train_X, train_Y)\nprediction3 = model.predict(test_X)\nprint('Accuracy of the Logistic Regression is ', metrics.accuracy_score(prediction3, test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:14.789745Z","iopub.execute_input":"2022-07-22T04:57:14.790929Z","iopub.status.idle":"2022-07-22T04:57:14.812632Z","shell.execute_reply.started":"2022-07-22T04:57:14.790898Z","shell.execute_reply":"2022-07-22T04:57:14.811854Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"### 3.4. Decision Tree","metadata":{}},{"cell_type":"code","source":"model = DecisionTreeClassifier()\nmodel.fit(train_X, train_Y)\nprediction4 = model.predict(test_X)\nprint('Accuracy of the Decision Tree is ', metrics.accuracy_score(prediction4, test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:14.813743Z","iopub.execute_input":"2022-07-22T04:57:14.814222Z","iopub.status.idle":"2022-07-22T04:57:14.827411Z","shell.execute_reply.started":"2022-07-22T04:57:14.814194Z","shell.execute_reply":"2022-07-22T04:57:14.826728Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"### 3.4. K-Nearest Neighbors(KNN)","metadata":{}},{"cell_type":"code","source":"model = KNeighborsClassifier()\nmodel.fit(train_X, train_Y)\nprediction5 = model.predict(test_X)\nprint('Accuracy of the KNN is ', metrics.accuracy_score(prediction5, test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:14.828420Z","iopub.execute_input":"2022-07-22T04:57:14.828907Z","iopub.status.idle":"2022-07-22T04:57:14.858563Z","shell.execute_reply.started":"2022-07-22T04:57:14.828879Z","shell.execute_reply":"2022-07-22T04:57:14.857166Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"K- 최근접 이웃 모델의 정확도는 n_neighbors의 값을 조절하면 변화한다. 기본값은 5이다. n_neighbor의 여러 값에 따른 정확도를 체크해보자.","metadata":{}},{"cell_type":"code","source":"a_index = list(range(1,11))\na = pd.Series()\nx = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nfor i in list(range(1,11)):\n    model = KNeighborsClassifier(n_neighbors=i)\n    model.fit(train_X, train_Y)\n    prediction = model.predict(test_X)\n    a = a.append(pd.Series(metrics.accuracy_score(prediction, test_Y)))\nplt.plot(a_index, a)\nplt.xticks(x)\nfig = plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\nprint('Accuracies for different values of n are : ', a.values, 'with the max values as', a.values.max())","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:14.859896Z","iopub.execute_input":"2022-07-22T04:57:14.860448Z","iopub.status.idle":"2022-07-22T04:57:15.213739Z","shell.execute_reply.started":"2022-07-22T04:57:14.860419Z","shell.execute_reply":"2022-07-22T04:57:15.212004Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"### 3.5. Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"model = GaussianNB()\nmodel.fit(train_X, train_Y)\nprediction6 = model.predict(test_X)\nprint('The accuracy of the Naive Bayes is ', metrics.accuracy_score(prediction6, test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:15.215421Z","iopub.execute_input":"2022-07-22T04:57:15.216207Z","iopub.status.idle":"2022-07-22T04:57:15.230047Z","shell.execute_reply.started":"2022-07-22T04:57:15.216127Z","shell.execute_reply":"2022-07-22T04:57:15.228778Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"## 3.6. Random Forest","metadata":{}},{"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X, train_Y)\nprediction7 = model.predict(test_X)\nprint('The accuracy of the Random Forests is ', metrics.accuracy_score(prediction7, test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:15.232254Z","iopub.execute_input":"2022-07-22T04:57:15.232604Z","iopub.status.idle":"2022-07-22T04:57:15.463179Z","shell.execute_reply.started":"2022-07-22T04:57:15.232574Z","shell.execute_reply":"2022-07-22T04:57:15.462128Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"모델의 정확도가 분류기의 rubustness를 결정하는 유일한 요소가 아니다.\n분류기가 훈련 데이터로 학습하고, 테스트 데이터로 테스트 했을 때, 정확도가 90%였다고 가정해보자.\n얼핏보면 분류기의 정확도가 매우 높은 것처럼 보여질 수 있다. 하지만 다른 테스트 세트에 대해서도 90%가 나올까?\n그렇지 않다. 분류기가 학습하기 위해 어떤 사건을 사용할지 결정할 수 없기 때문이다.\n훈련 데이터와 테스트 데이터가 변하면 정확도 역시 변하게 된다. 이를 Model Variance라고 한다. 이를 극복하고 일반화된 모델을 얻기 위해서는 우리가 Cross Validation(교차 검증)을 사용해야 한다.","metadata":{}},{"cell_type":"markdown","source":"## 4. Cross Validation(교차 검증)\n\n일반적으로 데이터들은 불균형하다. 많은 수의 class1 객체들이 존재하지만 다른 class 객체들은 그 수가 적을 수도 있다.\n그렇기 때문에 데이터셋 각각의 모든 객체에 알고리즘을 훈련시키고 테스트를 해야 한다.\n이 때 우리는 각 데이터셋에서 나온 정확도들의 평균을 이용할 수 있다.\n\n1) K-Fold Cross Validation에서는 먼저 데이터셋을 K개의 서브 데이터셋으로 나눈다.  \n2) 우리가 데이터셋을 5개로 나눴다고 하면, 1개의 데이터셋은 테스트용으로, 나머지 4개는 훈련용으로 사용한다.  \n3) 각 수행시마다 테스트셋을 바꿔주고, 다른 셋에 대해 알고리즘을 훈련시키면서 이 프로세스를 계속한다. 정확도와 오차는 평균화되어 알고리즘의 평균 정확도를 얻을 수 있다.  \n4) 일부 데이터 셋에서는 underfit(과소적합), 다른 데이터셋에서는 overfit(과대적합)이 될 수 있다.  ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold                   # K-Fold Cross Validation \nfrom sklearn.model_selection import cross_val_score         # 점수 평가\nfrom sklearn.model_selection import cross_val_predict       # 예측\n\nkfold = KFold(n_splits = 10, shuffle=True, random_state = 22) # k = 10 , 데이터셋을  동일 크기의 10개의 서브셋으로 나눕니다.\nxyz = []\naccuracy = []\nstd = []\nclassifiers = ['Linear Svm', 'Radial Svm', 'Logistic Regression', 'KNN', 'Decision Tree',\n              'Naive Bayes', 'Random Forest']\nmodels = [svm.SVC(kernel = 'linear'), svm.SVC(kernel = 'rbf'), LogisticRegression(), \n                 KNeighborsClassifier(n_neighbors=9), DecisionTreeClassifier(), GaussianNB(),\n                 RandomForestClassifier(n_estimators=100)]\nfor i in models :\n    model = i \n    cv_result = cross_val_score(model, X, Y, cv = kfold, scoring = 'accuracy')\n    cv_result = cv_result \n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\n\nnew_models_dataframe2 = pd.DataFrame({'CV Mean' : xyz, 'Std' : std}, index=classifiers)\nnew_models_dataframe2","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:15.465142Z","iopub.execute_input":"2022-07-22T04:57:15.465507Z","iopub.status.idle":"2022-07-22T04:57:17.781658Z","shell.execute_reply.started":"2022-07-22T04:57:15.465469Z","shell.execute_reply":"2022-07-22T04:57:17.781048Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"plt.subplots(figsize=(12,6))\nbox = pd.DataFrame(accuracy, index = [classifiers])\nbox.T.boxplot()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:17.782498Z","iopub.execute_input":"2022-07-22T04:57:17.783556Z","iopub.status.idle":"2022-07-22T04:57:18.002343Z","shell.execute_reply.started":"2022-07-22T04:57:17.783482Z","shell.execute_reply":"2022-07-22T04:57:18.001629Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"new_models_dataframe2['CV Mean'].plot.barh(width = 0.8)\nplt.title('Average CV Mean Accuracy')\nfig = plt.gcf()\nfig.set_size_inches(8,5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:18.003424Z","iopub.execute_input":"2022-07-22T04:57:18.004553Z","iopub.status.idle":"2022-07-22T04:57:18.169730Z","shell.execute_reply.started":"2022-07-22T04:57:18.004521Z","shell.execute_reply":"2022-07-22T04:57:18.168200Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"분류 정확도는 데이터 불균형으로 인해 잘못된 결론을 낼 수 있다. 혼동행렬을 이용해 요약된 결과를 얻을 수 있는데, 이 혼동행렬은 모델이 어디에서 잘못되었는지, 어떤 클래스를 모델이 잘못 예측했는지를 보여준다.","metadata":{}},{"cell_type":"markdown","source":"## 5. 혼동 행렬(Confusion Matrix)\n\n혼동행렬은 분류기에 의해 나온 정확한 혹은 부정확한 분류의 개수를 보여줍니다.","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(3,3, figsize=(12,10))\n\ny_pred = cross_val_predict(svm.SVC(kernel='rbf'), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred), ax=ax[0,0], annot=True, fmt = '2.0f')\nax[0,0].set_title('Matrix for rbf-SVM')\n\ny_pred = cross_val_predict(svm.SVC(kernel='linear'), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred), ax=ax[0,1], annot=True, fmt = '2.0f')\nax[0,1].set_title('Matrix for Linear-SVM')\n\ny_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred), ax=ax[0,2], annot=True, fmt = '2.0f')\nax[0,2].set_title('Matrix for KNN')\n\ny_pred = cross_val_predict(RandomForestClassifier(n_estimators=100), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred), ax=ax[1,0], annot=True, fmt = '2.0f')\nax[1,0].set_title('Matrix for Random-Forests')\n\ny_pred = cross_val_predict(LogisticRegression(), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred), ax=ax[1,1], annot=True, fmt = '2.0f')\nax[1,1].set_title('Matrix for Logistics Regression')\n\ny_pred = cross_val_predict(DecisionTreeClassifier(), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred), ax=ax[1,2], annot=True, fmt = '2.0f')\nax[1,2].set_title('Matrix for Decision Tree')\n\ny_pred = cross_val_predict(GaussianNB(), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred), ax=ax[2,0], annot=True, fmt = '2.0f')\nax[2,0].set_title('Matrix for Naive Bayes')\n\nplt.subplots_adjust(hspace=0.2, wspace = 0.2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:18.173562Z","iopub.execute_input":"2022-07-22T04:57:18.173917Z","iopub.status.idle":"2022-07-22T04:57:24.004631Z","shell.execute_reply.started":"2022-07-22T04:57:18.173889Z","shell.execute_reply":"2022-07-22T04:57:24.003534Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"#### 혼동행렬의 해석\n\n좌측상단-우측하단 대각선은 각 객체에 대해 정확한 예측의 수, 우측상단-좌측하단 대각선은 잘못된 예측의 수를 보여준다. 세로의 0과 1은 실제 결과에 대한 사망과 생존, 가로의 0과 1은 학습 모델이 판단한 사망과 생존에 대한 것. 이를 종합하면  \n\n좌측상단 - 실제 사망자를 학습 모델 역시 사망자로 분류  \n우측상단 - 실제 사망자를 학습 모델이 생존자로 분류  \n좌측하단 - 실제 생존자를 학습 모델이 사망자로 분류  \n우측하단 - 실제 생존자를 학습 모델 역시 생존자로 분류\n\n첫번째 plot의 rbf-SVM을 확인해보면,\n\n1) 정확한 예측의 수는 491(사망) + 247(생존)으로 평균 CV 정확도는  \n$$ 491+247 \\over 891 $$ = 82.8%이다.  \n\n2) Error(오류) : 58명의 사망자들이 생존자로 분류되었고, 95명의 생존자들이 사망자로 분류되었다는 뜻을 나타낸다.\n\n죽은 사람들을 살아있다고 예측하면서 많은 실수가 발생했다. 각각의 행렬을 보면 rbf-SVM이 사망자를 예측하는데 보다 정확하다고 볼 수 있다.  \n반면, Naive Bayes는 생존자를 예측하는데 보다 정확했다.\n이를 계산해보면,  \nrbf-SVM 사망자 예측 정확도  \n$$ 491 \\over 491+58 $$ = 89.4%(소수 둘째 자리에서 반올림)  \nNaive Bayes 생존자 예측 정확도  \n$$ 264 \\over 264+78 $$ = 77.2%(소수 둘째 자리에서 반올림)  \n\n3) 정확한 예측의 수는 442(사망), 264(생존)으로 평균 CV 정확도는\n$$ 442+264 \\over 891 $$ = 79.2%  \n\n4) 오류 : 107명의 사망자들이 생존자로 분류 및 78명의 생존자가 사망자로 분류됨.","metadata":{}},{"cell_type":"markdown","source":"## 6. 하이퍼 파라미터 튜닝\n\n머신러닝 모델은 블랙박스같다. 이 블랙박스에는 기본 파라미터 값이 있는데, 우리는 이것을 조절함으로써 더 좋은 모델을 얻을 수 있다. SVM 모델의 C와 gamma같이 다른 분류기에는 다른 파라미터들이 있는데 이를 하이퍼 파라미터라고 한다.\n이 하이퍼 파라미터를 튜닝해서 모델의 학습 능률을 변경해줄 수 있고 더 좋은 모델을 얻을 수 있다.\n좋은 결과를 보였던 2개의 분류기(SVM, Random Forest)의 하이퍼 파리미터를 튜닝해 보겠다.","metadata":{}},{"cell_type":"markdown","source":"### 6.1. SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nC = [0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\ngamma=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\nkernel=['rbf','linear']\nhyper={'kernel' : kernel, 'C' : C, 'gamma' : gamma}\ngd = GridSearchCV(estimator=svm.SVC(), param_grid=hyper, verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:57:24.006075Z","iopub.execute_input":"2022-07-22T04:57:24.006807Z","iopub.status.idle":"2022-07-22T04:58:15.227111Z","shell.execute_reply.started":"2022-07-22T04:57:24.006768Z","shell.execute_reply":"2022-07-22T04:58:15.226054Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"n_estimators = range(100, 1000, 100)\nhyper = { 'n_estimators' : n_estimators }\ngd = GridSearchCV(estimator=RandomForestClassifier(random_state=0), param_grid=hyper, verbose = True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:58:15.228407Z","iopub.execute_input":"2022-07-22T04:58:15.228731Z","iopub.status.idle":"2022-07-22T04:58:59.054227Z","shell.execute_reply.started":"2022-07-22T04:58:15.228678Z","shell.execute_reply":"2022-07-22T04:58:59.052878Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"Rbf-SVM의 최고 점수는 C=0.6일 때, gamma=0.1일 때인 82.82%이고, \nRandomForest는 n_estimators=400일 때인 81.82%이다.\n결과를 돌릴 때마다 학습 효율이 조금씩 달라질 수 있다는 점 참고하면 좋을 거 같다.","metadata":{}},{"cell_type":"markdown","source":"## 7. 앙상블(Ensembling)\n\n앙상블은 모델의 정확도와 성능을 높이기 위해 쓰는 방법이다.  \n하나의 강력한 모델을 만들기 위한 여러 단순한 모델의 조합이다.  \n핸드폰을 사기 위해 많은 사람들에게 여러 파라미터에 대해 질문했다고 가정해보면,  \n우리는 모든 파라미터들을 분석한 뒤 한 제품에 대한 강한 판단을 할 수 있을 것이다.\n이것이 모델의 안정성을 향상시켜주는 앙상블의 원리이며, 앙상블은 다음의 방법으로 수행할 수 있을 것이다.\n\n1) Voting Classifier  \n2) Bagging  \n3) Boosting  ","metadata":{}},{"cell_type":"markdown","source":"### 7.1. Voting Classifier\n\nVoting Classifier는 다양하면서도 단순한 학습 모델로부터의 예측들을 결합하는 가장 단순한 방법이다.\n예측값은 각 서브모델 예측치의 평균치이고, 각 서브모델들은 모두 다른 유형의 모델들이다.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nensemble_lin_rbf = VotingClassifier(estimators=[\n('KNN', KNeighborsClassifier(n_neighbors=10)),\n('RBF', svm.SVC(probability=True, kernel = 'rbf', C=0.5, gamma = 0.1)),\n('RFor', RandomForestClassifier(n_estimators=900, random_state=0)),\n('LR', LogisticRegression(C=0.05)),\n('DT', DecisionTreeClassifier(random_state=0)),\n('NB', GaussianNB()),\n('svm', svm.SVC(kernel='rbf', probability = True))],\nvoting='soft').fit(train_X, train_Y)\nprint('The accuracy for ensembled model is: ', ensemble_lin_rbf.score(test_X, test_Y))\ncross = cross_val_score(ensemble_lin_rbf, X, Y, cv = 10, scoring = 'accuracy')\nprint('The cross validated score is ', cross.mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:58:59.055820Z","iopub.execute_input":"2022-07-22T04:58:59.056104Z","iopub.status.idle":"2022-07-22T04:59:15.981812Z","shell.execute_reply.started":"2022-07-22T04:58:59.056080Z","shell.execute_reply":"2022-07-22T04:59:15.980642Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"### 7.2. Bagging\n\n베깅은 일반적인 앙상블 방법으로써 데이터셋의 작은 파티션에 대해 유사한 분류기들을 적용하고, 모든 예측치에 대한 평균을 적용시킨다. 그렇게 하면 평균화를 통해 분산이 감소가 되고, Voting Classifier와는 달리 배깅은 유사한 분류기를 사용합니다.","metadata":{}},{"cell_type":"markdown","source":"#### Bagged KNN\n\n베깅은 분산이 높은 모델에 가장 잘 작용 수 있다. 그 예는 Decision Tree나 Random Forests이다.\n우리는 n_neighbor의 작은 값을 적용하여 KNN을 n_neighbors의 작은 값으로 사용해 보고자 다.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nmodel = BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3), random_state=0, n_estimators=700)\nmodel.fit(train_X, train_Y)\nprediction = model.predict(test_X)\nprint('The accuracy for bagged KNN is : ', metrics.accuracy_score(prediction, test_Y))\nresult = cross_val_score(model, X, Y, cv=10, scoring='accuracy')\nprint('The cross validated score for bagged KNN is : ', result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:59:15.983285Z","iopub.execute_input":"2022-07-22T04:59:15.983586Z","iopub.status.idle":"2022-07-22T04:59:34.902363Z","shell.execute_reply.started":"2022-07-22T04:59:15.983555Z","shell.execute_reply":"2022-07-22T04:59:34.901509Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"### 7.3. Boosting\n\n부스팅은 분류기의 순차적인 학습을 이용한 앙상블 기법이다. 순차적으로 약한 모델을 향상시켜나간다.  \n부스팅은 아래와 같이 작동한다:  \n1) 모델은 처음 전체 데이터셋에 대해 학습한다. 이 때 모델은 일부 객체는 옳게, 일부 객체는 틀리게 예측할 것이다.  \n2) 그 다음 시행에서, 틀리게 예측한 객체에 더욱 가욱치를 둬서 학습한다. 결과적으로 틀리게 예측한 객체를 올바르게 예측하려고 노력한다.  \n3) 이런 과정이 반복되면서, 정확도가 한계에 도달할 때까지 새 분류기가 모델에 추가된다","metadata":{}},{"cell_type":"markdown","source":"#### AdaBoost(Adaptive Boosting)\n\n이번 케이스에서 약한 학습기는 Decision Tree이다. 하지만 우리는 기본 base_estimator를 우리의 선택에 따라 다른 알고리즘으로 바꿀 수 있다.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(n_estimators=200, random_state=0, learning_rate=0.1)\nresult = cross_val_score(ada, X, Y, cv=10, scoring='accuracy')\nprint('The cross validated score for AdaBoost is : ', result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:59:34.903451Z","iopub.execute_input":"2022-07-22T04:59:34.903736Z","iopub.status.idle":"2022-07-22T04:59:39.069323Z","shell.execute_reply.started":"2022-07-22T04:59:34.903692Z","shell.execute_reply":"2022-07-22T04:59:39.067837Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"#### Stochastic Gradient Boosting\n\n이번에도 약한 학습기는 Decision Tree이다.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngrad = GradientBoostingClassifier(n_estimators=500, random_state=0, learning_rate=0.1)\nresult = cross_val_score(grad, X, Y, cv=10, scoring='accuracy')\nprint('The cross validated score for Grandient Boosting is : ', result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:59:39.070919Z","iopub.execute_input":"2022-07-22T04:59:39.071289Z","iopub.status.idle":"2022-07-22T04:59:44.021860Z","shell.execute_reply.started":"2022-07-22T04:59:39.071248Z","shell.execute_reply":"2022-07-22T04:59:44.020744Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"#### XGBoost","metadata":{}},{"cell_type":"code","source":"import xgboost as xg\nxgboost = xg.XGBClassifier(n_estimators=900, learning_rate=0.1)\nresult = cross_val_score(xgboost, X, Y, cv=10, scoring='accuracy')\nprint('The cross validated score for XGBoost is : ', result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-22T04:59:44.023231Z","iopub.execute_input":"2022-07-22T04:59:44.023821Z","iopub.status.idle":"2022-07-22T05:00:04.898893Z","shell.execute_reply.started":"2022-07-22T04:59:44.023786Z","shell.execute_reply":"2022-07-22T05:00:04.898046Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"AdaBoost가 가장 높은 정확도를 기록하였다. 이 정확도를 하이퍼 파라미터 튜닝을 통해 더 높은 학습 효율을 이끌어 보겠다.","metadata":{}},{"cell_type":"code","source":"n_estimators = list(range(100, 1100, 100))\nlearning_rate = [0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\nhyper = {'n_estimators' : n_estimators, 'learning_rate' : learning_rate}\ngd = GridSearchCV(estimator=AdaBoostClassifier(), param_grid=hyper, verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-07-22T05:00:04.902349Z","iopub.execute_input":"2022-07-22T05:00:04.902906Z","iopub.status.idle":"2022-07-22T05:10:02.756744Z","shell.execute_reply.started":"2022-07-22T05:00:04.902878Z","shell.execute_reply":"2022-07-22T05:10:02.755839Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"#### 베스트 모델에 대한 혼동행렬","metadata":{}},{"cell_type":"code","source":"ada = AdaBoostClassifier(n_estimators=200, random_state=0,learning_rate = 0.05)\nresult = cross_val_predict(ada, X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y, result), cmap='winter', annot=True, fmt = '2.0f')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T05:10:02.758062Z","iopub.execute_input":"2022-07-22T05:10:02.759917Z","iopub.status.idle":"2022-07-22T05:10:05.935157Z","shell.execute_reply.started":"2022-07-22T05:10:02.759889Z","shell.execute_reply":"2022-07-22T05:10:05.934326Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"## 8. 특성 중요도(Feature Importance)","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(2,2, figsize=(15,12))\n\nmodel = RandomForestClassifier(n_estimators=500, random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8, ax=ax[0,0])\nax[0,0].set_title('Feature Importance in Random Forests')\n\nmodel=AdaBoostClassifier(n_estimators=200, learning_rate=0.05, random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8, ax=ax[0,1], color='#ddff11')\nax[0,1].set_title('Feature Importance in AdaBoost')\n\nmodel=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')\nax[1,0].set_title('Feature Importance in Gradient Boosting')\n\nmodel=xg.XGBClassifier(n_estimators=900, learning_rate=0.1)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')\nax[1,1].set_title('Feature Importance in XgBoost')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-22T05:10:05.936668Z","iopub.execute_input":"2022-07-22T05:10:05.937171Z","iopub.status.idle":"2022-07-22T05:10:10.614859Z","shell.execute_reply.started":"2022-07-22T05:10:05.937141Z","shell.execute_reply":"2022-07-22T05:10:10.613849Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"Random Forest, AdaBoost 등 여러 모델들에 대한 feature importance를 볼 수 있다.\n\n1) 공통적으로 중요한 feature는 Initial, Fare_cat, Pclass, Family_Size이다.  \n2) Sex는 그렇게 중요도가 높지 않아 보였는데, 앞선 분석에서 Pclass와 함께 보았을 때 성별이 중요한 요소였던 것을 생각하면 놀라운 결과이다.  \n3) 성별은 Random Forest 모델에서만 중요해 보인다. 하지만 많은 분류기의 최상단에 있는 Initial은 Sex와 양의 상관관계에 있다. 결국, 두 정보 모두 성별에 대한 정보를 담고 있다. \n4) 이와 비슷하게 Pclass와 Fare_cat 모두 탑승객의 지위와 Family_Size, Alone, Parch, SibSp의 정보를 담고 있다.","metadata":{}},{"cell_type":"markdown","source":"# 회고\n\n오늘 캐글 필사에서서 받은 첫 번째 느낌이다. 우선 필사 담당 퍼실님의 의도일지 모르겠지만 factorplot이 안 나오는 문제가 있었다고 한다. 하지만 이를 발견하고 pointplot으로 수정하는 작업을 거치면서 코드 작성의 정합성 및 오탈자 수정 같은 것에도 염두를 두고 써야겠구나 하면서 필사의 진가가 나오는 거 같다고 느꼈다. 단순히 복사 붙여넣기를 하면 손에 안 익기 때문에 발전이 더디기 때문에 직접 타이핑을 다 해봐야 한다라는 퍼실님의 말을 이제 이해할 수 있었다.  \n두 번째로 느낀 점은 주제가 기존에 있었던 타이타닉 생존자 판별 모델링에 대한 EDA였기 때문에 한번 더 복습을 할 수 있었다는 점이었다. 다만 K-최근접 이웃 모델, 교차 검증 같은 모델링에 관한 개념이나 혼동 행렬과 같은 시각화에 대한 새로운 개념들이 등장하여 타이핑을 하면서도 새삼 다른 내용이라는 것을 느낄 수 있었다. 혼동 행렬도 결국에는 생존자와 사망자를 얼마나 잘 맞췄는지를 시각화하여 누구나가 쉽게 이해할 수 있도록 하는 도구였으며, K-최근접 이웃 모델도 n_neighbors라는 하이퍼 파라미터를 통해 정확도를 조절이 가능하게 되는 일반적인 모델링과 크게 다르지 않다는 것을 깨달았다.  \n앞으로 캐글을 통해 더 다양한 모델을 돌리고 실험하는 과정을 거칠 것이라고 생각하니, 저번에 캐글 회고할 때와 마찬가지로 걱정 반 기대 반이었다.","metadata":{}}]}